import React from 'react';
import './References.scss';

const References = () => {
    return (
        <div className='references about-tab'>
            <div className='about-tab--section'>
                <div className='about-tab--section--header'>
                    Tham khảo
                </div>
                <div className='about-tab--section--body'>
                    <div className='about-tab--section--body--paragraph'>
                        <ul>
                            <li>
                                [Vie20] General Statistics Office of Vietnam. Completed Results of the 2019 Viet Nam
                                Population and Housing Census. 2020.
                            </li>
                            <li>
                                [CH13] Nathaniel CHEESEMAN and Jennifer HERINGTON. “Bahnaric linguistic bibliography with selected”. in(2013).
                            </li>
                            <li>
                                [Ngu+08] Loi Van Nguyen andothers. Chữ Bana Kriêm Bình Định. Ban dân tộc tỉnh
                                Bình Định, 2008.
                            </li>
                            <li>
                                [Jil+18] Y Jil andothers. Sổ tay phương ngữ tiếng Bahnar. Sở Giáo dục và Đào tạo tỉnh
                                Gia Lai, 2018.
                            </li>
                            <li>
                                [Hoà+98] Phê Hoàng andothers. “Vietnamese dictionary”. inScientific & Technical Pub-
                                lishing: (1998).
                            </li>
                            <li>
                                [Dou89] Pierre X Dourisboure. Dictionnaire bahnar-français. Société des missions étrangères,
                                1889.
                            </li>
                            <li>
                                [BB+79] John Banker, Elizabeth Banker andothers. Bahnar dictionary: Plei BongMang
                                Yang dialect. Summer Institute of Linguistics, Academic Publications, 1979.
                            </li>
                            <li>
                                [Vas+17] Ashish Vaswani andothers. “Attention is all you need”. inAdvances in neural
                                information processing systems: 2017, pages 5998–6008.
                            </li>
                            <li>
                                [He+16] Kaiming He andothers. “Deep residual learning for image recognition”. inProceedings
                                of the IEEE conference on computer vision and pattern recognition: 2016, pages 770–778.
                            </li>
                            <li>
                                [BKH16] Jimmy Lei Ba, Jamie Ryan Kiros and Geoffrey E Hinton. “Layer normalization”.
                                inarXiv preprint arXiv:1607.06450 : (2016).
                            </li>
                            <li>
                                [Dev+18] Jacob Devlin andothers. “Bert: Pre-training of deep bidirectional transformers
                                for language understanding”. inarXiv preprint arXiv:1810.04805 : (2018).
                            </li>
                            <li>
                                [DS13] Sanjay K Dwivedi and Vaishali Singh. “Research and reviews in question answering system”. inProcedia Technology: 10 (2013), pages 417–424.
                            </li>
                            <li>
                                [Mac09] Bill MacCartney. Natural language inference. Stanford University, 2009.
                            </li>
                            <li>
                                [Liu+19] Yinhan Liu andothers. “Roberta: A robustly optimized bert pretraining approach”. inarXiv preprint arXiv:1907.11692 : (2019).
                            </li>
                            <li>
                                [NN20] Dat Quoc Nguyen and Anh Tuan Nguyen. “PhoBERT: Pre-trained language
                                models for Vietnamese”. inarXiv preprint arXiv:2003.00744 : (2020).
                            </li>
                            <li>
                                [Lew+19] Mike Lewis andothers. “Bart: Denoising sequence-to-sequence pre-training for
                                natural language generation, translation, and comprehension”. inarXiv preprint
                                arXiv:1910.13461 : (2019).
                            </li>
                            <li>
                                [Liu+20] Yinhan Liu andothers. “Multilingual denoising pre-training for neural machine
                                translation”. inTransactions of the Association for Computational Linguistics: 8
                                (2020), pages 726–742.
                            </li>
                            <li>
                                [TLN21] Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen. “BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese”. inarXiv preprint: arXiv:2109.09701
                                (2021).
                            </li>
                            <li>
                                [Lin04] Chin-Yew Lin. “Rouge: A package for automatic evaluation of summaries”. inText
                                summarization branches out: 2004, pages 74–81.
                            </li>
                            <li>
                                [Zhu+20] Jinhua Zhu andothers. “Incorporating bert into neural machine translation”.
                                inarXiv preprint arXiv:2002.06823 : (2020).
                            </li>
                            <li>
                                [SLM17] Abigail See, Peter J Liu and Christopher D Manning. “Get to the point: Summarization with pointer-generator networks”. inarXiv preprint arXiv:1704.04368 :
                                (2017).
                            </li>
                            <li>
                                [BCB14] Dzmitry Bahdanau, Kyunghyun Cho and Yoshua Bengio. “Neural machine translation by jointly learning to align and translate”. inarXiv preprint arXiv:1409.0473 :
                                (2014).
                            </li>
                            <li>
                                [Dea+19] Jon Deaton andothers. “Transformers and pointer-generator networks for abstractive summarization”. in(2019).
                            </li>
                            <li>
                                [Edu+18] Sergey Edunov andothers. “Understanding back-translation at scale”. inarXiv
                                preprint arXiv:1808.09381 : (2018).
                            </li>
                            <li>
                                [CMH17] Anna Currey, Antonio Valerio Miceli-Barone and Kenneth Heafield. “Copied
                                monolingual data improves low-resource neural machine translation”. inProceedings
                                of the Second Conference on Machine Translation: 2017, pages 148–156.
                            </li>
                            <li>
                                [PPP12] Chirag Patel, Atul Patel and Dharmendra Patel. “Optical character recognition
                                by open source OCR tool tesseract: A case study”. inInternational Journal of
                                Computer Applications: 55.10 (2012), pages 50–56.
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    );
}

export default References;